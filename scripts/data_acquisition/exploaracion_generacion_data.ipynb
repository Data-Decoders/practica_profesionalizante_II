{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Origen de los datos**\n",
        "\n",
        "Los datos obtenidos para este proyecto corresponden a un conjunto de 500 imágenes de documentos de identidad. De estas, 250 imágenes representan documentos reales y las otras 250 son imágenes generadas artificialmente, donde se superponen datos falsos sobre documentos auténticos. Estos datos fueron recopilados de un servicio de almacenamiento en la nube llamado AWS.\n",
        "\n",
        "A continuación, se presenta un ejemplo de cómo se descargaron los datos desde Amazon S3 hacia un entorno local (por sencibilidad de datos y credenciales no se adjuntan solo se mencionan):\n",
        "\n",
        "\n",
        "```\n",
        "import boto3\n",
        "\n",
        "# Establecer las credenciales de AWS\n",
        "access_key = 'TU_ACCESS_KEY'\n",
        "secret_key = 'TU_SECRET_KEY'\n",
        "region_name = 'TU_REGION_NAME'\n",
        "\n",
        "# Crear una instancia del cliente de AWS S3\n",
        "s3_client = boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=region_name)\n",
        "\n",
        "# Nombre del bucket de S3 donde están almacenados los datos\n",
        "bucket_name = 'NOMBRE_DEL_BUCKET'\n",
        "\n",
        "# Directorio local donde se guardarán los datos descargados\n",
        "local_directory = 'DIRECTORIO_LOCAL'\n",
        "\n",
        "# Descargar los archivos desde S3 al directorio local\n",
        "response = s3_client.download_file(bucket_name, 'CARPETA_EN_S3/nombre_del_archivo.jpg', f'{local_directory}/nombre_del_archivo.jpg')\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Una vez que los datos se descargaron al entorno local, se organizaron en una carpeta compartida en Google Drive para facilitar el acceso y la colaboración entre los miembros del proyecto. Esto se hizo para que todos los involucrados pudieran acceder fácilmente a los datos y trabajar en conjunto en el desarrollo del proyecto.\n",
        "\n",
        "En resumen, se recopilaron 500 imágenes de documentos de identidad, de las cuales 250 son documentos reales y 250 son imágenes generadas artificialmente. Estos datos fueron obtenidos de AWS mediante una descarga desde Amazon S3 hacia un entorno local. Posteriormente, se organizaron en una carpeta compartida en Google Drive para facilitar el acceso y la colaboración en el proyecto."
      ],
      "metadata": {
        "id": "0gdhZo07N_i8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conexion a google drive"
      ],
      "metadata": {
        "id": "YwD3yz1LPzB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgXDvq5dQP9x",
        "outputId": "5e367f73-d22b-4d3b-92ba-66f70bc618d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 43.0 kB of archives.\n",
            "After this operation, 115 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tree amd64 1.8.0-1 [43.0 kB]\n",
            "Fetched 43.0 kB in 0s (125 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.8.0-1_amd64.deb ...\n",
            "Unpacking tree (1.8.0-1) ...\n",
            "Setting up tree (1.8.0-1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYqMDjI_JbLZ",
        "outputId": "1b89ee50-2911-45f0-fd2d-efe28ef5bca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#descarga del corpus\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_data  = '/content/drive/MyDrive/DIPLOMADO/DATA-SUPER'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DIPLOMADO/DATA-SUPER\n",
        "!tree -L 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGPjCMzoQcjg",
        "outputId": "f0c68020-ccd2-4e3a-86a4-c076ce2490d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DIPLOMADO/DATA-SUPER\n",
            "\u001b[01;34m.\u001b[00m\n",
            "├── \u001b[01;34mFALSE\u001b[00m\n",
            "└── \u001b[01;34mTRUE\u001b[00m\n",
            "\n",
            "2 directories, 0 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "def procesar_imagenes(ruta):\n",
        "    # Obtener la lista de archivos en la ruta\n",
        "    archivos = os.listdir(ruta)\n",
        "\n",
        "    # Filtrar solo los archivos de imagen (por extensión)\n",
        "    archivos_imagen = [archivo for archivo in archivos if archivo.lower().endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n",
        "\n",
        "    # Seleccionar aleatoriamente 5 imágenes\n",
        "    imagenes_seleccionadas = random.sample(archivos_imagen, 5)\n",
        "\n",
        "    for imagen in imagenes_seleccionadas:\n",
        "        # Leer la imagen\n",
        "        ruta_imagen = os.path.join(ruta, imagen)\n",
        "        img = Image.open(ruta_imagen)\n",
        "\n",
        "        # Leer el archivo XML asociado\n",
        "        ruta_xml = os.path.splitext(ruta_imagen)[0] + '.xml'\n",
        "        if os.path.isfile(ruta_xml):\n",
        "            # Parsear el archivo XML\n",
        "            tree = ET.parse(ruta_xml)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            # Obtener las coordenadas del recuadro\n",
        "            coordenadas = []\n",
        "            for obj in root.iter('object'):\n",
        "                xmin = int(obj.find('xmin').text)\n",
        "                ymin = int(obj.find('ymin').text)\n",
        "                xmax = int(obj.find('xmax').text)\n",
        "                ymax = int(obj.find('ymax').text)\n",
        "                coordenadas.append((xmin, ymin, xmax, ymax))\n",
        "\n",
        "            # Dibujar los recuadros en la imagen\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            for coord in coordenadas:\n",
        "                draw.rectangle(coord, outline='red')\n",
        "\n",
        "            # Aplicar un filtro de difuminado a las áreas fuera de los recuadros\n",
        "            blurred_img = img.filter(ImageFilter.BoxBlur(10))\n",
        "\n",
        "            # Mostrar la imagen original con los recuadros y la imagen difuminada\n",
        "            img.show()\n",
        "            blurred_img.show()\n",
        "        else:\n",
        "            print(f\"No se encontró el archivo XML asociado a la imagen: {imagen}\")\n"
      ],
      "metadata": {
        "id": "PnfcMrh_S_gO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.path.join(path_data,'TRUE'))\n",
        "procesar_imagenes(os.path.join(path_data,'TRUE'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSm87ZINTNgJ",
        "outputId": "771ac95e-95ad-495d-c180-dea6aa28ae59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DIPLOMADO/DATA-SUPER/TRUE\n",
            "No se encontró el archivo XML asociado a la imagen: 12166173K.jpg\n",
            "No se encontró el archivo XML asociado a la imagen: 74061955.jpg\n",
            "No se encontró el archivo XML asociado a la imagen: 13898365K.jpg\n",
            "No se encontró el archivo XML asociado a la imagen: front (46).png\n",
            "No se encontró el archivo XML asociado a la imagen: 43057305F.JPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "procesar_imagenes(os.path.join(path_data,'FALSE'))"
      ],
      "metadata": {
        "id": "gAhz-DJ5TV3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import argparse\n",
        "from lxml import etree\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "def generateDict(input_path):\n",
        "    dict_imgs = dict()\n",
        "    for file_i in os.listdir(input_path):\n",
        "        if '.ini' in file_i:\n",
        "            continue\n",
        "        if 'DS_Store' in file_i:\n",
        "            continue\n",
        "        if '.xml' not in file_i:\n",
        "            img_lst_i = file_i.split('.')\n",
        "            if len(img_lst_i) == 2:\n",
        "                name_img_i,formato_img_i = img_lst_i[0], img_lst_i[1]\n",
        "            else:\n",
        "                name_img_i, formato_img_i = \".\".join(img_lst_i[:len(img_lst_i)-1]), img_lst_i[len(img_lst_i)-1]\n",
        "            dict_imgs[name_img_i]=formato_img_i\n",
        "    return dict_imgs\n",
        "\n",
        "def get_region_coordinates(xml_path):\n",
        "    \"\"\"\n",
        "    Función que lee un archivo XML y devuelve las coordenadas de la región de interés.\n",
        "    \"\"\"\n",
        "    # Parseamos el archivo xml para obtener las coordenadas de recorte\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    xmin = int(root.find(\"./object/bndbox/xmin\").text)\n",
        "    ymin = int(root.find(\"./object/bndbox/ymin\").text)\n",
        "    xmax = int(root.find(\"./object/bndbox/xmax\").text)\n",
        "    ymax = int(root.find(\"./object/bndbox/ymax\").text)\n",
        "    print(xmin, ymin, xmax, ymax)\n",
        "    return xmin, ymin, xmax, ymax\n",
        "\n",
        "def main_cut(dir):\n",
        "    directorio_actual = os.path.dirname(os.path.abspath(__file__))\n",
        "    print(directorio_actual)\n",
        "\n",
        "    # Crea la carpeta cut si no existe\n",
        "    os.makedirs(os.path.join(dir,\"cut\"), exist_ok=True)\n",
        "    input_path = os.path.join(os.getcwd(), dir)\n",
        "    # os.makedirs(os.path.join(dir,\"cut\"), exist_ok=True)\n",
        "    dict_imgs=generateDict(input_path)\n",
        "\n",
        "    for name_i, formato_img_i in tqdm(dict_imgs.items(), total=len(dict_imgs)):\n",
        "        if name_i != '':\n",
        "    # for xml_file in xml_files:\n",
        "        # Obtiene la ruta de la imagen correspondiente\n",
        "            image_path = os.path.join(dir, name_i+'.'+formato_img_i)\n",
        "            print(image_path)\n",
        "\n",
        "            # Lee la imagen con OpenCV\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"Error: could not load image {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Obtiene las coordenadas de la región de interés\n",
        "            print(os.path.join(dir, name_i))\n",
        "            if not os.path.isfile(os.path.join(dir, name_i +\".xml\")):\n",
        "                continue\n",
        "            xmin, ymin, xmax, ymax = get_region_coordinates(os.path.join(dir, name_i +\".xml\"))\n",
        "            print(xmin, ymin, xmax, ymax)\n",
        "            # Recorta la imagen y guarda la imagen recortada en la carpeta cut\n",
        "            try:\n",
        "                if xmax-10 < image.shape[0] and   ymax-10 < image.shape[1]:\n",
        "                    cut_image = image[ymin-10:ymax+10, xmin-15:xmax+15]\n",
        "                else:\n",
        "                    cut_image = image[ymin:ymax, xmin:xmax]\n",
        "                cv2.imwrite(os.path.join(dir, \"cut\", name_i + \".\"+ formato_img_i), cut_image)\n",
        "            except:\n",
        "                cut_image = image[ymin:ymax, xmin:xmax]\n",
        "                cv2.imwrite(os.path.join(dir, \"cut\", name_i + \".\"+ formato_img_i), cut_image)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8lPH5F9FVUXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_cut()"
      ],
      "metadata": {
        "id": "fC8qSfdVXkEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funcion para generar de data set train, test y validation**\n",
        "\n",
        "Este generador de datos tomará las imágenes de las carpetas \"TRUE\" y \"FALSE\", las escalará a un tamaño de 250x250 píxeles, las normalizará y realizará aumentación de datos. Luego, generará conjuntos de entrenamiento, prueba y validación con las proporciones mencionadas, mezclando la información de las dos carpetas."
      ],
      "metadata": {
        "id": "QvQKmOPpYyoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def create_data_generator(data_dir, target_size=(250, 250), batch_size=32):\n",
        "    # Crear generador de aumentación de datos\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0 / 255.0,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Obtener las rutas de las imágenes de las carpetas \"TRUE\" y \"FALSE\"\n",
        "    true_dir = os.path.join(data_dir, \"TRUE\")\n",
        "    false_dir = os.path.join(data_dir, \"FALSE\")\n",
        "\n",
        "    # Obtener las listas de archivos de las carpetas\n",
        "    true_files = [os.path.join(true_dir, file) for file in os.listdir(true_dir)]\n",
        "    false_files = [os.path.join(false_dir, file) for file in os.listdir(false_dir)]\n",
        "\n",
        "    # Mezclar las listas de archivos\n",
        "    all_files = true_files + false_files\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    # Calcular el número de muestras en cada conjunto (train, test, validación)\n",
        "    num_samples = len(all_files)\n",
        "    num_train = int(0.6 * num_samples)\n",
        "    num_test = int(0.2 * num_samples)\n",
        "    num_val = num_samples - num_train - num_test\n",
        "\n",
        "    # Dividir las rutas de los archivos en conjuntos de train, test y validación\n",
        "    train_files = all_files[:num_train]\n",
        "    test_files = all_files[num_train:num_train+num_test]\n",
        "    val_files = all_files[num_train+num_test:]\n",
        "\n",
        "    # Crear generadores de datos para cada conjunto\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    return train_generator, test_generator, val_generator\n",
        "\n",
        "# Ruta que contiene las carpetas \"TRUE\" y \"FALSE\" con las imágenes\n",
        "data_dir = \"ruta_a_tu_carpeta_principal\"\n",
        "\n",
        "# Crear el generador de datos\n",
        "train_generator, test_generator, val_generator = create_data_generator(data_dir)\n"
      ],
      "metadata": {
        "id": "3TN84abKYtZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}